{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from glob import glob\n",
    "from joblib import dump, load\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the african country dictionary\n",
    "dict_path = \"data/output/african_countries.json\"\n",
    "\n",
    "with open(dict_path) as json_file:\n",
    "    ccDict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all features\n",
    "with open('data/output/mil_exp.json') as json_file:\n",
    "    mil_exp = json.load(json_file)\n",
    "    \n",
    "with open('data/output/population.json') as json_file:\n",
    "    population = json.load(json_file)\n",
    "    \n",
    "with open('data/output/arms_imports.json') as json_file:\n",
    "    arms_imports = json.load(json_file)\n",
    "    \n",
    "with open('data/output/conflicts.json') as json_file:\n",
    "    conflicts = json.load(json_file)\n",
    "    \n",
    "with open('data/output/mil_pers.json') as json_file:\n",
    "    mil_pers = json.load(json_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the time windows we will work with\n",
    "minYear = 1962\n",
    "maxYear = 2012\n",
    "\n",
    "# Go through all datasets\n",
    "temp_df = []\n",
    "for i, key in enumerate(ccDict):\n",
    "    \n",
    "    # Get the country name\n",
    "    country_name = ccDict[key]['name']\n",
    "    \n",
    "    # Go through years\n",
    "    for year in range(minYear, maxYear+1):\n",
    "    \n",
    "        # datum\n",
    "        datum = {\n",
    "            'COW_key': int(key),\n",
    "            'Year': int(year),\n",
    "            'Mil_Exp':float(mil_exp[key][str(year)]),\n",
    "            'Population':int(population[key][str(year)]),\n",
    "            'Mil_Pers':float(mil_pers[key][str(year)]),\n",
    "            'Arms_Imports':int(arms_imports[key][str(year)]),\n",
    "            'Conflict':conflicts[key][str(year)]\n",
    "        }\n",
    "        \n",
    "        # Append to temp df\n",
    "        temp_df.append(datum)\n",
    "\n",
    "# Convert temp df to pandas\n",
    "df = pd.DataFrame(temp_df) \n",
    "    \n",
    "\n",
    "# Print nbr of rows\n",
    "print(\"Nbr of rows : \" + str(len(df.index)))\n",
    "\n",
    "# Preview df\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_milexp = df.mean(axis = 0)['Mil_Exp']\n",
    "mean_milpers = df.mean(axis = 0)['Mil_Pers']\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    if(row['Mil_Exp'] == 0):\n",
    "        df.at[index,'Mil_Exp'] = mean_milexp\n",
    "    \n",
    "    if(row['Mil_Pers'] == 0):\n",
    "        df.at[index,'Mil_Pers'] = mean_milpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle Rows\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# count excess\n",
    "imbalance = df['Conflict'].value_counts()\n",
    "\n",
    "excessLabel = 0\n",
    "if(imbalance[0] > imbalance[1]):\n",
    "    excessLabel = 0\n",
    "else:\n",
    "    excessLabel = 1\n",
    "\n",
    "# Nbr of excess\n",
    "diff = abs(imbalance[0] - imbalance[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = df.copy()\n",
    "\n",
    "nbr_dropped = 0\n",
    "for index, row in balanced_df.iterrows():\n",
    "    \n",
    "    if(nbr_dropped >= diff):\n",
    "        break\n",
    "    \n",
    "    if(row['Conflict'] == excessLabel):\n",
    "        balanced_df.drop(index, inplace=True)\n",
    "        nbr_dropped += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df['Conflict'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features/label\n",
    "features = ['Arms_Imports', 'Mil_Exp', 'Mil_Pers', 'Population', 'Year']\n",
    "label = ['Conflict']\n",
    "X = balanced_df[features]\n",
    "y = balanced_df[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(X, y, test_size=0.01, random_state=12, shuffle=True, stratify=y)\n",
    "\n",
    "# cast to np\n",
    "valid_Y = np.array(valid_Y)\n",
    "valid_X = np.array(valid_X)\n",
    "\n",
    "print(\"Length of training set : \", len(train_X))\n",
    "print(\"Length of validation set : \", len(valid_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rdf_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "rdf_classifier.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_predictions = rdf_classifier.predict(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = 0\n",
    "for i, pred in enumerate(rdf_predictions):\n",
    "    if(pred == valid_Y[i]):\n",
    "        success += 1\n",
    "        \n",
    "print(\"Validation Accuracy = \" + str(success/len(train_Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "train_X_t_n = scaler.fit_transform(train_X)\n",
    "valid_X_t_n = scaler.transform(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_classifier = LogisticRegression()\n",
    "log_classifier.fit(train_X_t_n, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_predictions = log_classifier.predict(valid_X_t_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = 0\n",
    "for i, pred in enumerate(log_predictions):\n",
    "    if(pred == valid_Y[i]):\n",
    "        success += 1\n",
    "        \n",
    "print(\"Validation Accuracy = \" + str(success/len(train_Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm_classifier = svm.SVC(gamma='auto',probability=True)\n",
    "svm_classifier.fit(train_X_t_n, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = svm_classifier.predict(valid_X_t_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = 0\n",
    "for i, pred in enumerate(svm_predictions):\n",
    "    if(pred == valid_Y[i]):\n",
    "        success += 1\n",
    "        \n",
    "print(\"Validation Accuracy = \" + str(success/len(train_Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(rdf_classifier, 'data/model/model.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataviz Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init a dict that will contain the total value of arms import per year per country\n",
    "predict_dict = {}\n",
    "\n",
    "nbrOfKey = len(ccDict.keys())\n",
    "\n",
    "for i, key in tqdm(enumerate(ccDict), total=nbrOfKey):\n",
    "    predict_dict[key] = {}\n",
    "    \n",
    "    for year in range(minYear, maxYear+1):\n",
    "        \n",
    "        # create datum in SAME order\n",
    "        datum = [arms_imports[key][str(year)], float(mil_exp[key][str(year)]), float(mil_pers[key][str(year)]),int(population[key][str(year)]),year]\n",
    "        \n",
    "        # predict with probabibility\n",
    "        predict_dict[key][str(year)] = str(rdf_classifier.predict_proba([datum])[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dict to a json file\n",
    "with open('data/output/predictions.json', 'w') as fp:\n",
    "    json.dump(predict_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
