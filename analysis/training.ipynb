{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from joblib import dump, load\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the african country dictionary\n",
    "dict_path = \"data/output/african_countries.json\"\n",
    "\n",
    "with open(dict_path) as json_file:\n",
    "    ccDict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all features\n",
    "with open('data/output/mil_exp.json') as json_file:\n",
    "    mil_exp = json.load(json_file)\n",
    "    \n",
    "with open('data/output/population.json') as json_file:\n",
    "    population = json.load(json_file)\n",
    "    \n",
    "with open('data/output/arms_imports.json') as json_file:\n",
    "    arms_imports = json.load(json_file)\n",
    "    \n",
    "with open('data/output/mil_pers.json') as json_file:\n",
    "    mil_pers = json.load(json_file)\n",
    "    \n",
    "with open('data/output/water.json') as json_file:\n",
    "    water = json.load(json_file)\n",
    "    \n",
    "with open('data/output/surface.json') as json_file:\n",
    "    surface = json.load(json_file)\n",
    "    \n",
    "    \n",
    "with open('data/output/conflicts.json') as json_file:\n",
    "    conflicts = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the time windows we will work with\n",
    "minYear = 1962\n",
    "maxYear = 2012\n",
    "\n",
    "# Go through all datasets\n",
    "temp_df = []\n",
    "for i, key in enumerate(ccDict):\n",
    "    \n",
    "    # Get the country name\n",
    "    country_name = ccDict[key]['name']\n",
    "    \n",
    "    # Go through years\n",
    "    for year in range(minYear, maxYear+1):\n",
    "    \n",
    "        # datum\n",
    "        datum = {\n",
    "            'COW_key': int(key),\n",
    "            'Year': int(year),\n",
    "            'Mil_Exp':float(mil_exp[key][str(year)]),\n",
    "            'Population':int(population[key][str(year)]),\n",
    "            'Mil_Pers':float(mil_pers[key][str(year)]),\n",
    "            'Arms_Imports':int(arms_imports[key][str(year)]),\n",
    "            'Water':float(water[key][str(year)]),\n",
    "            'Surface':float(surface[key][str(year)]),\n",
    "            'Conflict':conflicts[key][str(year)]\n",
    "        }\n",
    "        \n",
    "        # Append to temp df\n",
    "        temp_df.append(datum)\n",
    "\n",
    "# Convert temp df to pandas\n",
    "df = pd.DataFrame(temp_df) \n",
    "    \n",
    "\n",
    "# Print nbr of rows\n",
    "print(\"Nbr of rows : \" + str(len(df.index)))\n",
    "\n",
    "# Preview df\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_milexp = df.mean(axis = 0)['Mil_Exp']\n",
    "mean_milpers = df.mean(axis = 0)['Mil_Pers']\n",
    "mean_water = df.mean(axis = 0)['Water']\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    if(row['Mil_Exp'] == 0):\n",
    "        df.at[index,'Mil_Exp'] = mean_milexp\n",
    "    \n",
    "    if(row['Mil_Pers'] == 0):\n",
    "        df.at[index,'Mil_Pers'] = mean_milpers\n",
    "    \n",
    "    if(row['Water'] == 0):\n",
    "        df.at[index,'Water'] = mean_water"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle Rows\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# count excess\n",
    "imbalance = df['Conflict'].value_counts()\n",
    "\n",
    "excessLabel = 0\n",
    "if(imbalance[0] > imbalance[1]):\n",
    "    excessLabel = 0\n",
    "else:\n",
    "    excessLabel = 1\n",
    "\n",
    "# Nbr of excess\n",
    "diff = abs(imbalance[0] - imbalance[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = df.copy()\n",
    "\n",
    "nbr_dropped = 0\n",
    "for index, row in balanced_df.iterrows():\n",
    "    \n",
    "    if(nbr_dropped >= diff):\n",
    "        break\n",
    "    \n",
    "    if(row['Conflict'] == excessLabel):\n",
    "        balanced_df.drop(index, inplace=True)\n",
    "        nbr_dropped += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df['Conflict'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features/label\n",
    "features = ['Arms_Imports', 'Mil_Exp', 'Mil_Pers', 'Population', 'Water', 'Surface']\n",
    "label = ['Conflict']\n",
    "X = balanced_df[features]\n",
    "y = balanced_df[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(X, y, test_size=0.01, random_state=42, shuffle=True, stratify=y)\n",
    "\n",
    "# cast to np\n",
    "valid_Y = np.array(valid_Y)\n",
    "valid_X = np.array(valid_X)\n",
    "\n",
    "print(\"Length of training set : \", len(train_X))\n",
    "print(\"Length of validation set : \", len(valid_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "train_X_n = scaler.fit_transform(train_X)\n",
    "valid_X_n = scaler.transform(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Scaler\n",
    "scaler_filename = \"data/model/scaler.joblib\"\n",
    "dump(scaler, scaler_filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rdf_classifier = RandomForestClassifier(n_estimators=30, random_state=0)\n",
    "rdf_classifier.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_predictions = rdf_classifier.predict(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = 0\n",
    "for i, pred in enumerate(rdf_predictions):\n",
    "    if(pred == valid_Y[i]):\n",
    "        success += 1\n",
    "        \n",
    "print(\"Validation Accuracy = \" + str(success/len(valid_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_classifier = LogisticRegression()\n",
    "log_classifier.fit(train_X_n, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_predictions = log_classifier.predict(valid_X_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = 0\n",
    "for i, pred in enumerate(log_predictions):\n",
    "    if(pred == valid_Y[i]):\n",
    "        success += 1\n",
    "        \n",
    "print(\"Validation Accuracy = \" + str(success/len(valid_X_n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm_classifier = svm.SVC(gamma='auto',probability=True)\n",
    "svm_classifier.fit(train_X_n, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = svm_classifier.predict(valid_X_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = 0\n",
    "for i, pred in enumerate(svm_predictions):\n",
    "    if(pred == valid_Y[i]):\n",
    "        success += 1\n",
    "        \n",
    "print(\"Validation Accuracy = \" + str(success/len(valid_X_n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(rdf_classifier, 'data/model/model.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=len(features), activation= \"relu\"))\n",
    "model.add(Dense(60, activation= \"relu\"))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(60, activation= \"relu\"))\n",
    "model.add(Dense(30, activation= \"relu\"))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary() #Print model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss=\"binary_crossentropy\" , optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "history = model.fit(train_X_n, train_Y, epochs=125, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(valid_X_n, valid_Y)\n",
    "\n",
    "print('Test Score: {}'.format(score[0]))\n",
    "print('Test Accuracy: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('Training')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss','accuracy'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_predictions = model.predict(valid_X_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = 0\n",
    "for i, pred in enumerate(cnn_predictions):\n",
    "    if(round(pred[0]) == valid_Y[i]):\n",
    "        success += 1\n",
    "        \n",
    "print(\"Validation Accuracy = \" + str(success/len(valid_X_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('data/model/model_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models\n",
    "try:\n",
    "    rdf_classifier = load('data/model/model.joblib')\n",
    "    print(\"RDF classifier loaded!\")\n",
    "except:\n",
    "    print(\"ERROR: RDF not loaded\")\n",
    "    \n",
    "try:\n",
    "    cnn_classifier = load_model('data/model/model_cnn.h5')\n",
    "    print(\"CNN classifier loaded!\")\n",
    "except:\n",
    "    print(\"ERROR: CNN not loaded\")\n",
    "    \n",
    "# Load Scaler\n",
    "try:\n",
    "    scaler = load('data/model/scaler.joblib')\n",
    "    print(\"Scaler loaded!\")\n",
    "except:\n",
    "    print(\"ERROR: Scaler not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagged_predict(x, weight_rdf, weight_cnn):\n",
    "    \n",
    "    # Scale\n",
    "    x_n = scaler.transform(x)\n",
    "    \n",
    "    # get prediction from the models\n",
    "    rdf_preds = rdf_classifier.predict_proba(x)\n",
    "    cnn_preds = model.predict(x_n)\n",
    "    \n",
    "    # init array\n",
    "    predictions = []\n",
    "    \n",
    "    # go through\n",
    "    for i in range(0,len(x)):\n",
    "        \n",
    "        # get preds\n",
    "        rdf_pred = rdf_preds[i]\n",
    "        cnn_pred = cnn_preds[i][0]\n",
    "    \n",
    "        # different rdf scenarios\n",
    "        if(rdf_pred[0] > rdf_pred[1]):\n",
    "            rdf_pred = 1.0 - rdf_pred[0]\n",
    "\n",
    "        elif(rdf_pred[1] > rdf_pred[0]):\n",
    "            rdf_pred = rdf_pred[1]\n",
    "\n",
    "        else:\n",
    "            rdf_pred = 0.5\n",
    "    \n",
    "        # get weighted average \n",
    "        ave_pred = rdf_pred*weight_rdf + cnn_pred*weight_cnn\n",
    "        \n",
    "        # add to final array\n",
    "        predictions.append(ave_pred)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestRDF_weight = 0\n",
    "bestRDF_maxAcc = 0\n",
    "\n",
    "for j in range(-10,11):\n",
    "    \n",
    "    weight_rdf = 0.5 + j/20\n",
    "    weight_cnn = 0.5 - j/20\n",
    "    \n",
    "    success = 0\n",
    "    \n",
    "    # Get bagged prediction\n",
    "    preds = bagged_predict(valid_X, weight_rdf, weight_cnn)\n",
    "    \n",
    "    for i, pred in enumerate(preds):\n",
    "\n",
    "        # count successes\n",
    "        if(round(pred) == valid_Y[i]):\n",
    "            success += 1\n",
    "\n",
    "    \n",
    "    # Compute Score\n",
    "    score = success/len(valid_X_n)\n",
    "    \n",
    "    if(score > bestRDF_maxAcc):\n",
    "        bestRDF_maxAcc = score\n",
    "        bestRDF_weight = weight_rdf\n",
    "        \n",
    "print(\"Validation Accuracy = \" + str(bestRDF_maxAcc))\n",
    "print(\"Best RDF Weight = \" + str(bestRDF_weight))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bagged prediction\n",
    "preds = bagged_predict(valid_X, weight_rdf, weight_cnn)\n",
    "success = 0\n",
    "for i, pred in enumerate(preds):\n",
    "\n",
    "    # count successes\n",
    "    if(round(pred) == valid_Y[i]):\n",
    "        success += 1\n",
    "        \n",
    "\n",
    "print(\"Validation Accuracy = \" + str(success/len(valid_Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataviz Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(0.9,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init a dict that will contain the total value of arms import per year per country\n",
    "predict_dict = {}\n",
    "\n",
    "nbrOfKey = len(ccDict.keys())\n",
    "\n",
    "for i, key in tqdm(enumerate(ccDict), total=nbrOfKey):\n",
    "    predict_dict[key] = {}\n",
    "    \n",
    "    for year in range(minYear, maxYear+1):\n",
    "        \n",
    "        # create datum in SAME order\n",
    "        datum = [\n",
    "             arms_imports[key][str(year)], \n",
    "             float(mil_exp[key][str(year)]), \n",
    "             float(mil_pers[key][str(year)]),\n",
    "             int(population[key][str(year)]), \n",
    "             float(water[key][str(year)]),\n",
    "             float(surface[key][str(year)]),\n",
    "        ]\n",
    "        \n",
    "        # predict with probabibility\n",
    "        prediction = model.predict([datum])[0]\n",
    "        \n",
    "        # set value\n",
    "        predict_dict[key][str(year)] = str(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dict to a json file\n",
    "with open('data/output/predictions.json', 'w') as fp:\n",
    "    json.dump(predict_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
